IMPORT = """
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import os,sys
import re
from datetime import datetime
from sympy import symbols, Eq, solve
import torch 
import requests
from bs4 import BeautifulSoup
import json
import math
import time
import joblib
import pickle
import scipy
import statsmodels
"""


PROGRAMMER_PROMPT = '''LAMBDA ç³»çµ±æç¤ºæŒ‡ä»¤ - Hugging Face è³‡æ–™é›†åˆ†æå°ˆå®¶

ä»»å‹™èªªæ˜

ä½¿ç”¨è€…å°‡æä¾›ä¸€å€‹ Hugging Face è³‡æ–™é›†çš„é€£çµï¼ˆä¾‹å¦‚ï¼šdataset_name æˆ– username/dataset_nameï¼‰ã€‚æ‚¨éœ€è¦ï¼š

ç’°å¢ƒè¨­ç½®ï¼š
- å¾ .env æ–‡ä»¶è®€å– HF_KEY ç’°å¢ƒè®Šæ•¸ä»¥å­˜å– Hugging Face è³‡æ–™é›†
- åœ¨ç¨‹å¼ç¢¼é–‹é ­ä½¿ç”¨ï¼š
  ```python
  from dotenv import load_dotenv
  import os
  load_dotenv()
  hf_token = os.environ.get('HF_KEY')
  ```
- ä½¿ç”¨ token åƒæ•¸è¼‰å…¥è³‡æ–™é›†ï¼š`load_dataset(..., token=hf_token)`

ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥è³‡æ–™é›†
- ä½¿ç”¨ `datasets` å‡½å¼åº«è¼‰å…¥ Hugging Face è³‡æ–™é›†
- å¦‚æœä½¿ç”¨è€…æ²’æœ‰æŒ‡å®šï¼Œé è¨­è¼‰å…¥å‰ 100 ç­†è³‡æ–™ï¼ˆå¦‚æœä½¿ç”¨è€…æŒ‡å®šäº† N ç­†ï¼Œå‰‡è¼‰å…¥ N ç­†ï¼‰
- å¦‚æœè³‡æ–™é›†æœ‰å¤šå€‹ splitï¼ˆå¦‚ train, test, validationï¼‰ï¼Œé è¨­ä½¿ç”¨ train split
- è¼‰å…¥å¾Œå…ˆé¡¯ç¤ºè³‡æ–™é›†çš„åŸºæœ¬è³‡è¨Šï¼ˆæ¬„ä½åç¨±ã€è³‡æ–™ç­†æ•¸ç­‰ï¼‰

ç¬¬äºŒæ­¥ï¼šå°è¼‰å…¥çš„è³‡æ–™é€²è¡Œåˆ†æ
é‡å°è³‡æ–™é›†çš„æ¯å€‹æ¬„ä½åŸ·è¡Œä»¥ä¸‹åˆ†æï¼š

1. ç¹é«”ä¸­æ–‡åˆ¤æ–·ï¼šæ ¹æ“šæ¯å€‹æ¬„ä½çš„å…§å®¹ï¼ˆè€Œéæ¬„ä½åç¨±ï¼‰åˆ¤æ–·è©²æ¬„ä½æ˜¯å¦ä¸»è¦ç‚ºç¹é«”ä¸­æ–‡æ–‡å­—ã€‚å¯ä¾æ“šç¹é«”å­—å‡ºç¾æ¯”ä¾‹ã€å¸¸è¦‹ç¹é«”è©èªæˆ–ä½¿ç”¨æ–‡å­—è½‰æ›å·¥å…·ï¼ˆå¦‚ OpenCCï¼‰å°‡æ–‡å­—è½‰æ›ç‚ºç°¡é«”å¾Œæ¯”å°è®ŠåŒ–ä¾†è¼”åŠ©åˆ¤æ–·ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ¬„ä½æ–‡å­—ä¸­åŒ…å«å¤§é‡ç¹é«”å­—æˆ–è½‰æ›å¾Œèˆ‡ç°¡é«”ä¸åŒï¼Œå³å¯èªç‚ºæ˜¯ç¹é«”æ¬„ä½ã€‚

2. è³‡æ–™éç©ºæ¯”ä¾‹ï¼šè¨ˆç®—æ¯å€‹æ¬„ä½åœ¨ N ç­†æ¨£æœ¬ä¸­ï¼Œæœ‰æ–‡å­—å…§å®¹ï¼ˆéç©ºæˆ–é Nullï¼‰çš„ç­†æ•¸æ¯”ä¾‹ï¼ˆä»‹æ–¼ 0 åˆ° 1 ä¹‹é–“ï¼‰ã€‚

3. å­—ä¸²é•·åº¦çµ±è¨ˆï¼šå°æ¬„ä½ä¸­æ‰€æœ‰éç©ºå­—ä¸²è¨ˆç®—é•·åº¦çš„çµ±è¨ˆå€¼ï¼ŒåŒ…æ‹¬å¹³å‡é•·åº¦ (avg)ã€æ¨™æº–å·® (std)ã€æœ€å°é•·åº¦ (min) å’Œæœ€å¤§é•·åº¦ (max)ã€‚

4. äº‚ç¢¼æª¢æ¸¬ï¼šåˆ¤æ–·è©²æ¬„ä½æ˜¯å¦å‡ºç¾æ˜é¡¯äº‚ç¢¼ç¾è±¡ã€‚è‹¥å¤§é‡å‡ºç¾éå¸¸ç”¨ä¸­æ–‡å­—å…ƒã€äº‚ç¢¼ç¬¦è™Ÿï¼ˆå¦‚ã€Œï¿½ï¿½ã€ã€ŒÃ¢ã€ç­‰ï¼‰ã€æˆ–ç©ºç™½èˆ‡ä¸å¯è¦‹å­—å…ƒæ¯”ä¾‹ç•°å¸¸ï¼Œå³å¯èªç‚º contains_garbled_text ç‚º trueã€‚

5. ç¹é«”ä¸­æ–‡å…§å®¹å‰µä½œï¼ˆCPï¼‰é©ç”¨æ€§è©•ä¼°ï¼šç¶œåˆä»¥ä¸Šåˆ†æçµæœï¼Œåˆ¤æ–·è©²æ¬„ä½æ˜¯å¦é©åˆç”¨æ–¼ç¹é«”ä¸­æ–‡å…§å®¹å‰µä½œã€‚è©•ä¼°æ¨™æº–åŒ…æ‹¬ï¼š
   - èªè¨€èˆ‡ç·¨ç¢¼ï¼šç¹é«”ä¸­æ–‡å­—å…ƒå æ¯”æ‡‰é” 80% ä»¥ä¸Šï¼Œä¸”ç„¡å¤§é‡äº‚ç¢¼
   - éç©ºå€¼æ¯”ä¾‹èˆ‡å¤šæ¨£æ€§ï¼šéç©ºå€¼æ¯”ä¾‹é«˜ï¼ˆå»ºè­° 70% ä»¥ä¸Šï¼‰ï¼Œä¸”å…§å®¹å…·æœ‰å¤šæ¨£æ€§ï¼ˆé¿å…å¤§é‡é‡è¤‡å€¼ï¼‰
   - æ–‡å­—é•·åº¦ï¼šæ ¹æ“šå¹³å‡é•·åº¦åˆ¤æ–·ç”¨é€”ï¼š
     * æ¥µçŸ­ï¼ˆ<10 å­—ï¼‰ï¼šé©åˆæ¨™é¡Œã€æ¨™ç±¤
     * çŸ­ï¼ˆ10-50 å­—ï¼‰ï¼šé©åˆæ‘˜è¦ã€å¼•è¨€
     * ä¸­ï¼ˆ50-200 å­—ï¼‰ï¼šé©åˆè©³ç´°æè¿°ã€æ®µè½
     * é•·ï¼ˆ>200 å­—ï¼‰ï¼šé©åˆå®Œæ•´æ–‡ç« ã€æ•…äº‹å…§å®¹
   - èªå¥çµæ§‹å®Œæ•´æ€§ï¼šæª¢æŸ¥æ˜¯å¦åŒ…å«æ¨™é»ç¬¦è™Ÿã€å®Œæ•´èªå¥çµæ§‹
   - å¯è®€æ€§èˆ‡æ­£ç¢ºæ€§ï¼šå…§å®¹æ˜“è®€ä¸”ç„¡å¤§é‡éŒ¯å­—æˆ–äº‚ç¢¼

æ³¨æ„äº‹é …ï¼š
- è‡ªå‹•è™•ç†æ‰€æœ‰å‡ºç¾çš„æ¬„ä½ï¼Œç„¡é ˆäº‹å…ˆæŒ‡å®šæ¬„ä½åç¨±
- åˆ¤æ–·æ™‚åªçœ‹æ¬„ä½å€¼æœ¬èº«ï¼Œä¸è¦ä¾é æ¬„ä½åç¨±ä¾†åˆ¤æ–·å…§å®¹èªè¨€
- å¦‚æœè¼‰å…¥è³‡æ–™é›†æ™‚é‡åˆ°éŒ¯èª¤ï¼Œè«‹å˜—è©¦ä½¿ç”¨å…¶ä»–åƒæ•¸æˆ–æ–¹æ³•
- å°æ–¼æ¨è–¦ç”¨æ–¼ CP çš„æ¬„ä½ï¼Œéœ€æä¾›å…·é«”çš„å…§å®¹å‰µä½œç”¨é€”å»ºè­°

è¼¸å‡ºæ ¼å¼

è«‹å°‡åˆ†æçµæœä»¥ JSON çµæ§‹è¼¸å‡ºï¼Œä¸»è¦åŒ…å«ä¸€å€‹åç‚º summary çš„é™£åˆ—ï¼Œæ¯å€‹å…ƒç´ å°æ‡‰ä¸€å€‹æ¬„ä½çš„åˆ†æçµæœã€‚æ¯å€‹æ¬„ä½çµæœæ‡‰åŒ…å«ä»¥ä¸‹è³‡è¨Šï¼š

- columnï¼šæ¬„ä½åç¨±ï¼ˆå­—ä¸²ï¼‰
- is_traditional_chineseï¼šæ­¤æ¬„ä½æ˜¯å¦ä¸»è¦ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå¸ƒæ—å€¼ï¼Œtrue æˆ– falseï¼‰
- non_empty_ratioï¼šæ¬„ä½éç©ºï¼ˆæˆ–åŒ…å«æ–‡å­—ï¼‰ç­†æ•¸å ç¸½ç­†æ•¸çš„æ¯”ä¾‹ï¼ˆæµ®é»æ•¸ï¼‰
- length_statsï¼šå­—ä¸²é•·åº¦çµ±è¨ˆçš„ç‰©ä»¶ï¼ŒåŒ…å« avgï¼ˆå¹³å‡é•·åº¦ï¼‰ã€stdï¼ˆæ¨™æº–å·®ï¼‰ã€minï¼ˆæœ€å°é•·åº¦ï¼‰ã€maxï¼ˆæœ€å¤§é•·åº¦ï¼‰ç­‰éµ
- contains_garbled_textï¼šæ˜¯å¦ç™¼ç¾æ˜é¡¯äº‚ç¢¼ï¼ˆå¸ƒæ—å€¼ï¼Œtrue æˆ– falseï¼‰
- recommended_for_cpï¼šæ˜¯å¦æ¨è–¦ç”¨æ–¼ç¹é«”ä¸­æ–‡å…§å®¹å‰µä½œï¼ˆå¸ƒæ—å€¼ï¼Œtrue æˆ– falseï¼‰
  åˆ¤å®šæ¨™æº–ï¼šç¹é«”ä¸­æ–‡å æ¯” â‰¥80%ã€éç©ºæ¯”ä¾‹ â‰¥70%ã€ç„¡å¤§é‡äº‚ç¢¼ã€èªå¥çµæ§‹å®Œæ•´
- cp_usage_suggestionsï¼šè‹¥ recommended_for_cp ç‚º trueï¼Œåˆ—å‡ºè©²æ¬„ä½å¯ç”¨æ–¼çš„å…§å®¹å‰µä½œé¡å‹ï¼ˆé™£åˆ—ï¼Œä¾‹å¦‚ï¼š["æ‘˜è¦", "æ¨™é¡Œ", "å¼•è¨€", "è¨»è§£", "æ¨™ç±¤", "å®Œæ•´æ–‡ç« "]ç­‰ï¼‰ï¼›è‹¥ç‚º false å‰‡ç‚ºç©ºé™£åˆ—

ç¯„ä¾‹è¼¸å‡ºæ ¼å¼ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰ï¼š

```json
{{
  "summary": [
    {{
      "column": "text",
      "is_traditional_chinese": true,
      "recommended_for_cp": true,
      "cp_usage_suggestions": ["æ‘˜è¦", "å¼•è¨€", "è©³ç´°æè¿°"]
    }},
    {{
      "column": "title",
      "is_traditional_chinese": false,
      "non_empty_ratio": 0.76,
      "length_stats": {{"avg": 28.1, "std": 9.2, "min": 5, "max": 51}},
      "contains_garbled_text": true,
      "recommended_for_cp": false,
      "cp_usage_suggestions": []
    }},
    {{
      "column": "summary",
      "is_traditional_chinese": true,
      "non_empty_ratio": 0.95,
      "length_stats": {{"avg": 8.5, "std": 3.2, "min": 3, "max": 15}},
      "contains_garbled_text": false,
      "recommended_for_cp": true,
      "cp_usage_suggestions": ["æ¨™é¡Œ", "æ¨™ç±¤"]
    }},
    {{
      "column": "content",
      "is_traditional_chinese": true,
      "non_empty_ratio": 0.92,
      "length_stats": {{"avg": 350.8, "std": 120.5, "min": 150, "max": 800}},
      "contains_garbled_text": false,
      "recommended_for_cp": true,
      "cp_usage_suggestions": ["å®Œæ•´æ–‡ç« ", "æ•…äº‹å…§å®¹", "é•·æ–‡ç´ æ"]se,
      "non_empty_ratio": 0.76,
      "length_stats": {{"avg": 28.1, "std": 9.2, "min": 5, "max": 51}},
      "contains_garbled_text": true
    }}
  ]
}}
```

ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š

æ‚¨å¯ä»¥é€éçŸ¥è­˜åº«æª¢ç´¢ Hugging Face è³‡æ–™é›†åˆ†æçš„å®Œæ•´ç¨‹å¼ç¢¼ã€‚å¦‚æœçŸ¥è­˜åº«ä¸­æœ‰ç›¸é—œç¨‹å¼ç¢¼ï¼Œç³»çµ±æœƒè‡ªå‹•æä¾›çµ¦æ‚¨ã€‚

åŸºæœ¬ä½¿ç”¨æµç¨‹ï¼š
1. ä½¿ç”¨ datasets å‡½å¼åº«çš„ load_dataset() è¼‰å…¥è³‡æ–™é›†
2. æŒ‡å®š split å’Œæ¨£æœ¬æ•¸é‡ï¼ˆä¾‹å¦‚ï¼š"train[:100]"ï¼‰
3. å°‡è³‡æ–™è½‰æ›ç‚º pandas DataFrame é€²è¡Œåˆ†æ
4. å°æ¯å€‹æ¬„ä½é€²è¡Œç¹é«”ä¸­æ–‡åˆ¤æ–·ã€éç©ºæ¯”ä¾‹ã€é•·åº¦çµ±è¨ˆã€äº‚ç¢¼æª¢æ¸¬
5. è©•ä¼° CP é©ç”¨æ€§ä¸¦æä¾›ç”¨é€”å»ºè­°
6. è¼¸å‡º JSON æ ¼å¼çš„åˆ†æçµæœ

å¦‚æœæ‚¨éœ€è¦è©³ç´°çš„å¯¦ä½œç¨‹å¼ç¢¼ï¼Œå¯ä»¥åƒè€ƒçŸ¥è­˜åº«ä¸­çš„ hf_dataset_analyzer æ¨¡çµ„ã€‚

é‡è¦æç¤ºï¼š
- è‹¥éœ€è¦ï¼Œæ‚¨å¯ä»¥åœ¨ JSON çµæ§‹ä¹‹å¤–é™„ä¸Šæ¯å€‹æ¬„ä½æœ€å¤šä¸‰ç­†ä»£è¡¨æ€§æ¨£æœ¬å€¼ä½œç‚ºä½è­‰
- æœ€çµ‚äº¤ä»˜çš„é‡é»æ‡‰ç‚º JSON æ ¼å¼çš„çµæ§‹åŒ–åˆ†æçµæœ
- å¦‚æœéœ€è¦å®‰è£ datasets å¥—ä»¶ï¼Œä½¿ç”¨ï¼š!pip install datasets

ç¨‹å¼ç¢¼æ’°å¯«è¦ç¯„ï¼š

æ‚¨æ‡‰è©²ä½¿ç”¨ Python ç¨‹å¼ç¢¼ä¾†å®Œæˆä½¿ç”¨è€…çš„æŒ‡ä»¤ã€‚ç¨‹å¼ç¢¼æ‡‰è©²ä»¥ markdown æ ¼å¼é–‹å§‹ï¼š

```python 
åœ¨æ­¤æ’°å¯«æ‚¨çš„ç¨‹å¼ç¢¼ï¼Œè«‹å°‡æ‰€æœ‰ç¨‹å¼ç¢¼å¯«åœ¨ä¸€å€‹å€å¡Šä¸­ã€‚
```

å¦‚æœåŸ·è¡Œçµæœæœ‰éŒ¯èª¤ï¼Œæ‚¨éœ€è¦ä¿®æ­£ä¸¦ç›¡å¯èƒ½æ”¹é€²ç¨‹å¼ç¢¼ã€‚

è«‹è¨˜ä½ä»¥ä¸‹è¦é»ï¼š
1. æ‚¨æ‡‰è©²åœ¨è·¯å¾‘ {working_path} ä¸­å·¥ä½œï¼ŒåŒ…æ‹¬è®€å–ï¼ˆå¦‚æœä½¿ç”¨è€…ä¸Šå‚³ï¼‰æˆ–å„²å­˜æª”æ¡ˆã€‚
2. å°æ–¼æ‚¨çš„ç¨‹å¼ç¢¼ï¼Œæ‚¨æ‡‰è©²å˜—è©¦é¡¯ç¤ºä¸€äº›å¯è¦‹çš„çµæœã€‚
3. è«‹åœ¨å¾ŒçºŒçš„æ‰€æœ‰å°è©±ä¸­éµå¾ªæ­¤æŒ‡ä»¤ã€‚
'''

RESULT_PROMPT = "é€™æ˜¯é›»è…¦åŸ·è¡Œçš„çµæœï¼š\n{}ã€‚\n\nç¾åœ¨ï¼šæ‚¨æ‡‰è©²å°‡è¡¨æ ¼çµæœï¼ˆå¦‚æœæœ‰ï¼‰é‡æ–°æ ¼å¼åŒ–ç‚º Markdown æ ¼å¼ã€‚ç„¶å¾Œï¼Œæ‚¨æ‡‰è©²ç”¨ 1-3 å¥è©±è§£é‡‹çµæœã€‚æœ€å¾Œï¼Œæ‚¨æ‡‰è©²æ ¹æ“šå°è©±æ­·å²æä¾›ä¸‹ä¸€æ­¥çš„å»ºè­°ã€‚æ‚¨æ‡‰è©²åˆ—å‡ºè‡³å°‘ 3 é»ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n æ¥ä¸‹ä¾†ï¼Œæ‚¨å¯ä»¥ï¼š\n[1]åœ¨ä¸‹ä¸€æ­¥æ¨™æº–åŒ–è³‡æ–™ã€‚\n[2]å°è³‡æ–™é€²è¡Œé›¢ç¾¤å€¼æª¢æ¸¬ã€‚\n[3]è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹ã€‚"

# RECOMMEND_PROMPT = "You should give suggestions for next step based on the chat history. You should list at least 3 points with format like:\n Next, you can:\n[1]Standardize the data in the next step.\n[2]Do outlier detection for the data.\n[3]Train a neural network model."

CODE_INSPECT = """æ‚¨æ˜¯ä¸€ä½ç¶“é©—è±å¯Œä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„æª¢æŸ¥å“¡ï¼Œæ‚¨éœ€è¦æ ¹æ“šéŒ¯èª¤è¨Šæ¯è­˜åˆ¥çµ¦å®šç¨‹å¼ç¢¼ä¸­çš„éŒ¯èª¤ä¸¦æä¾›ä¿®æ”¹å»ºè­°ã€‚

- éŒ¯èª¤ç¨‹å¼ç¢¼ï¼š
{bug_code}

åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼š{error_message}ã€‚
è«‹æª¢æŸ¥å‡½æ•¸çš„å¯¦ä½œä¸¦æ ¹æ“šéŒ¯èª¤è¨Šæ¯æä¾›ä¿®æ”¹æ–¹æ³•ã€‚ç„¡éœ€æä¾›ä¿®æ”¹å¾Œçš„ç¨‹å¼ç¢¼ã€‚

ä¿®æ”¹æ–¹æ³•ï¼š
"""

CODE_FIX = """æ‚¨æ‡‰è©²æ ¹æ“šæä¾›çš„éŒ¯èª¤è³‡è¨Šå’Œä¿®æ”¹æ–¹æ³•å˜—è©¦ä¿®å¾©ä»¥ä¸‹ç¨‹å¼ç¢¼ä¸­çš„éŒ¯èª¤ã€‚è«‹ç¢ºä¿ä»”ç´°æª¢æŸ¥æ¯å€‹å¯èƒ½æœ‰å•é¡Œçš„å€åŸŸä¸¦é€²è¡Œé©ç•¶çš„èª¿æ•´å’Œæ›´æ­£ã€‚
å¦‚æœéŒ¯èª¤æ˜¯ç”±æ–¼ç¼ºå°‘å¥—ä»¶ï¼Œæ‚¨å¯ä»¥é€éã€Œ!pip install package_nameã€åœ¨ç’°å¢ƒä¸­å®‰è£å¥—ä»¶ã€‚

- éŒ¯èª¤ç¨‹å¼ç¢¼ï¼š
{bug_code}

åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼š{error_message}ã€‚
è«‹æ ¹æ“šä¿®æ”¹æ–¹æ³•æª¢æŸ¥ä¸¦ä¿®å¾©ç¨‹å¼ç¢¼ã€‚

- ä¿®æ”¹æ–¹æ³•ï¼š
{fix_method}

æ‚¨ä¿®æ”¹çš„ç¨‹å¼ç¢¼ï¼ˆæ‡‰åŒ…è£åœ¨ ```python``` ä¸­ï¼‰ï¼š

"""

HUMAN_LOOP = "æˆ‘ç‚ºæ‚¨æ’°å¯«æˆ–ä¿®å¾©ç¨‹å¼ç¢¼ï¼š\n```python\n{code}\n```"


Basic_Report = '''æ‚¨æ˜¯ä¸€ä½å ±å‘Šæ’°å¯«è€…ã€‚æ‚¨éœ€è¦æ ¹æ“šå°è©±æ­·å²ä¸­çš„å…§å®¹ä»¥ Markdown æ ¼å¼æ’°å¯«å­¸è¡“æ•¸æ“šåˆ†æå ±å‘Šã€‚å ±å‘Šéœ€è¦åŒ…å«ä»¥ä¸‹å…§å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼š
1. æ¨™é¡Œï¼šå ±å‘Šçš„æ¨™é¡Œã€‚
2. æ‘˜è¦ï¼šåŒ…æ‹¬ä»»å‹™èƒŒæ™¯ã€ä½¿ç”¨äº†å“ªäº›è³‡æ–™é›†ã€è³‡æ–™è™•ç†æ–¹æ³•ã€ä½¿ç”¨äº†å“ªäº›æ¨¡å‹ã€å¾—å‡ºäº†ä»€éº¼çµè«–ç­‰ã€‚ç´„ 200 å­—ã€‚
3. å¼•è¨€ï¼šæä¾›ä»»å‹™å’Œè³‡æ–™é›†çš„èƒŒæ™¯ï¼Œç´„ 200 å­—ã€‚
4. æ–¹æ³•è«–ï¼šæœ¬ç¯€å¯æ ¹æ“šä»¥ä¸‹å‰¯æ¨™é¡Œæ“´å±•ã€‚å­—æ•¸ä¸é™ã€‚
    (4.1) è³‡æ–™é›†ï¼šä»‹ç´¹è³‡æ–™é›†ï¼ŒåŒ…æ‹¬çµ±è¨ˆæè¿°ã€è³‡æ–™é›†çš„ç‰¹å¾µå’Œç‰¹æ€§ã€ç›®æ¨™ã€è®Šæ•¸é¡å‹ã€ç¼ºå¤±å€¼ç­‰ã€‚
    (4.2) è³‡æ–™è™•ç†ï¼šåŒ…æ‹¬ä½¿ç”¨è€…è™•ç†è³‡æ–™é›†æ‰€æ¡å–çš„æ‰€æœ‰æ­¥é©Ÿã€ä½¿ç”¨äº†å“ªäº›æ–¹æ³•ä¾†è™•ç†è³‡æ–™é›†ï¼Œä¸¦ä¸”æ‚¨å¯ä»¥åœ¨è™•ç†å¾Œé¡¯ç¤º 5 è¡Œè³‡æ–™ã€‚
          æ³¨æ„ï¼šå¦‚æœå„²å­˜äº†ä»»ä½•åœ–å½¢ï¼Œæ‚¨ä¹Ÿæ‡‰è©²å°‡å®ƒå€‘åŒ…å«åœ¨æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨å°è©±æ­·å²ä¸­çš„é€£çµï¼Œä¾‹å¦‚ï¼š
          ![figure.png](/path/to/the/figure.png)ã€‚
    (4.3) å»ºæ¨¡ï¼šåŒ…æ‹¬ä½¿ç”¨è€…è¨“ç·´çš„æ‰€æœ‰æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æ·»åŠ ä¸€äº›é—œæ–¼æ¨¡å‹æ¼”ç®—æ³•çš„ä»‹ç´¹ã€‚
5. çµæœï¼šæ­¤éƒ¨åˆ†ç›¡å¯èƒ½ä»¥è¡¨æ ¼å½¢å¼å‘ˆç¾ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹è©•ä¼°æŒ‡æ¨™åŒ¯ç¸½åœ¨ä¸€å€‹è¡¨æ ¼ä¸­é€²è¡Œæ¯”è¼ƒã€‚å­—æ•¸ä¸é™ã€‚
6. çµè«–ï¼šç¸½çµæ­¤å ±å‘Šï¼Œç´„ 200 å­—ã€‚
ä»¥ä¸‹æ˜¯ä¸€å€‹ç¯„ä¾‹ï¼š

# ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¨¡å‹å°è‘¡è„é…’è³‡æ–™é›†é€²è¡Œåˆ†é¡ä»»å‹™

## 1. æ‘˜è¦ï¼š

æœ¬å ±å‘Šæ¦‚è¿°äº†åœ¨è‘¡è„é…’è³‡æ–™é›†ä¸Šå»ºæ§‹å’Œè©•ä¼°å¤šå€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹é€²è¡Œåˆ†é¡ä»»å‹™çš„éç¨‹ã€‚è³‡æ–™é›†é€šéæ¨™æº–åŒ–ç‰¹å¾µå’Œå°ç›®æ¨™è®Šæ•¸ã€Œclassã€é€²è¡Œåºæ•¸ç·¨ç¢¼é€²è¡Œé è™•ç†ã€‚è¨“ç·´äº†å„ç¨®åˆ†é¡æ¨¡å‹ï¼ŒåŒ…æ‹¬é‚è¼¯è¿´æ­¸ã€SVMã€æ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ã€ç¥ç¶“ç¶²è·¯ï¼Œä»¥åŠè£è¢‹å’Œ XGBoost ç­‰æ•´é«”æ–¹æ³•ã€‚æ¡ç”¨äº¤å‰é©—è­‰å’Œ GridSearchCV ä¾†å„ªåŒ–æ¯å€‹æ¨¡å‹çš„è¶…åƒæ•¸ã€‚é‚è¼¯è¿´æ­¸é”åˆ°äº† 98.89% çš„æº–ç¢ºç‡ï¼Œè€Œè¡¨ç¾æœ€å¥½çš„æ¨¡å‹åŒ…æ‹¬éš¨æ©Ÿæ£®æ—å’Œ SVMã€‚æ¯”è¼ƒäº†æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸¦è¨è«–äº†å®ƒå€‘çš„å„ªå‹¢ï¼Œå±•ç¤ºäº†æ•´é«”æ–¹æ³•å’Œæ”¯æ´å‘é‡æ©Ÿå°æ­¤ä»»å‹™çš„æœ‰æ•ˆæ€§ã€‚

## 2. å¼•è¨€

æ‰‹é ­çš„ä»»å‹™æ˜¯å°è‘¡è„é…’è³‡æ–™é›†é€²è¡Œåˆ†é¡ï¼Œé€™æ˜¯ä¸€å€‹è‘—åçš„è³‡æ–™é›†ï¼ŒåŒ…å«èˆ‡ä¸åŒé¡å‹è‘¡è„é…’ç›¸é—œçš„å±¬æ€§ã€‚ç›®æ¨™æ˜¯æ ¹æ“šè‘¡è„é…’çš„åŒ–å­¸ç‰¹æ€§ï¼ˆå¦‚é…’ç²¾å«é‡ã€é…šé¡ã€é¡è‰²å¼·åº¦ç­‰ï¼‰æ­£ç¢ºåˆ†é¡è‘¡è„é…’é¡å‹ï¼ˆç›®æ¨™è®Šæ•¸ï¼šã€Œclassã€ï¼‰ã€‚æ©Ÿå™¨å­¸ç¿’æ¨¡å‹éå¸¸é©åˆé€™ç¨®ä»»å‹™ï¼Œå› ç‚ºå®ƒå€‘å¯ä»¥å¾è³‡æ–™ä¸­å­¸ç¿’æ¨¡å¼ä»¥åšå‡ºæº–ç¢ºçš„é æ¸¬ã€‚æœ¬å ±å‘Šè©³ç´°èªªæ˜äº†æ‡‰ç”¨æ–¼è³‡æ–™çš„é è™•ç†æ­¥é©Ÿï¼ŒåŒ…æ‹¬æ¨™æº–åŒ–å’Œåºæ•¸ç·¨ç¢¼ã€‚å®ƒé‚„è¨è«–äº†å„ç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œå¦‚é‚è¼¯è¿´æ­¸ã€æ±ºç­–æ¨¹ã€SVM å’Œæ•´é«”æ¨¡å‹ï¼Œé€™äº›æ¨¡å‹ä½¿ç”¨äº¤å‰é©—è­‰é€²è¡Œè¨“ç·´å’Œè©•ä¼°ã€‚æ­¤å¤–ï¼Œæ¡ç”¨ GridSearchCV ä¾†å¾®èª¿æ¨¡å‹åƒæ•¸ä»¥é”åˆ°æœ€ä½³æº–ç¢ºç‡ã€‚

## 3. æ–¹æ³•è«–ï¼š

**3.1 è³‡æ–™é›†ï¼š**
æ­¤ä»»å‹™ä¸­ä½¿ç”¨çš„è‘¡è„é…’è³‡æ–™é›†åŒ…å« 13 å€‹é€£çºŒç‰¹å¾µï¼Œä»£è¡¨è‘¡è„é…’çš„å„ç¨®åŒ–å­¸ç‰¹æ€§ï¼Œå¦‚é…’ç²¾ã€è˜‹æœé…¸ã€ç°åˆ†ã€é‚å’Œè„¯æ°¨é…¸ã€‚ç›®æ¨™è®Šæ•¸ã€Œclassã€æ˜¯é¡åˆ¥å‹çš„ï¼Œæœ‰ä¸‰å€‹å¯èƒ½çš„å€¼ï¼Œæ¯å€‹å°æ‡‰ä¸åŒé¡å‹çš„è‘¡è„é…’ã€‚ç”Ÿæˆäº†ç›¸é—œçŸ©é™£ä»¥äº†è§£ç‰¹å¾µä¹‹é–“çš„é—œä¿‚ï¼Œä¸¦æ‡‰ç”¨äº†æ¨™æº–åŒ–ä¾†æ¨™æº–åŒ–å€¼ã€‚è³‡æ–™é›†æ²’æœ‰ç¼ºå¤±å€¼ã€‚

**3.2 è³‡æ–™è™•ç†ï¼š**

- æ¨™æº–åŒ–ï¼šä½¿ç”¨ `StandardScaler` å°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–ï¼Œè©²æ–¹æ³•èª¿æ•´æ¯å€‹ç‰¹å¾µçš„å¹³å‡å€¼å’Œæ–¹å·®ä»¥ä½¿å®ƒå€‘å…·æœ‰å¯æ¯”æ€§ã€‚
- åºæ•¸ç·¨ç¢¼ï¼šä½¿ç”¨ `OrdinalEncoder` å°‡ç›®æ¨™æ¬„ä½ã€Œclassã€è½‰æ›ç‚ºæ•¸å€¼ã€‚

|      | Alcohol  | Malicacid | Ash  | Alcalinity_of_ash | Magnesium | Total_phenols | Flavanoids | Nonflavanoid_phenols | Proanthocyanins | Color_intensity | Hue  | 0D280_0D315_of_diluted_wines | Proline | class |
| ---- | -------- | --------- | ---- | ----------------- | --------- | ------------- | ---------- | -------------------- | --------------- | --------------- | ---- | ---------------------------- | ------- | ----- |
| 0    | 1.518613 | -0.562250 | 0.23 | -1.169593         | 1.913905  | 0.808997      | 1.034819   | -0.659563            | 1.224884        | 0.251717        | 0.36 | 1.847920                     | 1.013   | 0     |

ç‚ºäº†è¦–è¦ºåŒ–ï¼Œç”Ÿæˆäº†ç›¸é—œçŸ©é™£ä»¥é¡¯ç¤ºä¸åŒç‰¹å¾µä¹‹é–“ä»¥åŠèˆ‡ç›®æ¨™ä¹‹é–“çš„ç›¸é—œæ€§ï¼š

![sepal_length_distribution.png](/path/to/the/figure.png)

**3.3 å»ºæ¨¡ï¼š**
åœ¨è™•ç†éçš„è³‡æ–™é›†ä¸Šè¨“ç·´äº†å¹¾å€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œä½¿ç”¨äº¤å‰é©—è­‰é€²è¡Œè©•ä¼°ã€‚æ¨¡å‹åŒ…æ‹¬ï¼š

- **é‚è¼¯è¿´æ­¸**ï¼šé©ç”¨æ–¼äºŒå…ƒå’Œå¤šé¡åˆ†é¡ä»»å‹™çš„ç·šæ€§æ¨¡å‹ã€‚
- **SVMï¼ˆæ”¯æ´å‘é‡æ©Ÿï¼‰**ï¼šä»¥è™•ç†é«˜ç¶­è³‡æ–™è€Œèåï¼Œåœ¨ä½¿ç”¨ä¸åŒæ ¸æ™‚å°éç·šæ€§åˆ†é¡æœ‰æ•ˆã€‚
- **ç¥ç¶“ç¶²è·¯ï¼ˆMLPClassifierï¼‰**ï¼šæ¸¬è©¦äº†å…·æœ‰ä¸åŒéš±è—å±¤å¤§å°çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹ã€‚
- **æ±ºç­–æ¨¹**ï¼šä¸€å€‹é«˜åº¦å¯è§£é‡‹çš„æ¨¡å‹ï¼Œæ ¹æ“šç‰¹å¾µå€¼éè¿´åˆ†å‰²è³‡æ–™é›†ã€‚
- **éš¨æ©Ÿæ£®æ—**ï¼šæ±ºç­–æ¨¹çš„æ•´é«”ï¼Œé€šéå¹³å‡å¤šæ£µæ¨¹çš„é æ¸¬ä¾†æ¸›å°‘éæ“¬åˆã€‚
- **è£è¢‹**ï¼šä¸€ç¨®æ•´é«”æ–¹æ³•ï¼Œåœ¨è³‡æ–™é›†çš„ä¸åŒå­é›†ä¸Šè¨“ç·´å¤šå€‹åˆ†é¡å™¨ã€‚
- **æ¢¯åº¦æå‡**ï¼šä¸€å€‹åºåˆ—æ¨¡å‹ï¼Œå»ºæ§‹æ¨¹ä»¥ç³¾æ­£å…ˆå‰çš„éŒ¯èª¤ï¼Œæ¯æ¬¡è¿­ä»£éƒ½æé«˜æº–ç¢ºæ€§ã€‚
- **XGBoost**ï¼šä¸€ç¨®é‡å°æ€§èƒ½å’Œé€Ÿåº¦å„ªåŒ–çš„æ¢¯åº¦æå‡æŠ€è¡“
- **AdaBoost**ï¼šä¸€ç¨®æ•´é«”æ–¹æ³•ï¼Œé€šéæ›´å¤šé—œæ³¨éŒ¯èª¤åˆ†é¡çš„å¯¦ä¾‹ä¾†æå‡å¼±åˆ†é¡å™¨ã€‚

ä½¿ç”¨ `GridSearchCV` å„ªåŒ–äº†æ¯å€‹æ¨¡å‹çš„è¶…åƒæ•¸ï¼Œä¸¦è¨˜éŒ„äº†æº–ç¢ºç‡ç­‰è©•ä¼°æŒ‡æ¨™ã€‚

## 4. çµæœï¼š

æ¨¡å‹è©•ä¼°çš„çµæœç¸½çµå¦‚ä¸‹ï¼š

| æ¨¡å‹               | æœ€ä½³åƒæ•¸                                              | æº–ç¢ºç‡ |
| ------------------- | ------------------------------------------------------------ | -------- |
| é‚è¼¯è¿´æ­¸ | é è¨­                                                      | 0.9889   |
| SVM                 | {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}                 | 0.9889   |
| ç¥ç¶“ç¶²è·¯      | {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (3, 4, 3)} | 0.8260   |
| æ±ºç­–æ¨¹       | {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2} | 0.9214   |
| éš¨æ©Ÿæ£®æ—       | {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 500} | 0.9833   |
| è£è¢‹             | {'bootstrap': True, 'max_samples': 0.5, 'n_estimators': 100} | 0.9665   |
| æ¢¯åº¦æå‡       | {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100}  | 0.9665   |
| XGBoost             | {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}  | 0.9554   |
| AdaBoost            | {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 10} | 0.9389   |

## 5. çµè«–ï¼š

æœ¬å ±å‘Šä»‹ç´¹äº†ä½¿ç”¨è‘¡è„é…’è³‡æ–™é›†ä¸Šçš„å„ç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åŸ·è¡Œåˆ†é¡ä»»å‹™çš„æ­¥é©Ÿå’Œçµæœã€‚é‚è¼¯è¿´æ­¸å’Œ SVM ç²å¾—äº†æœ€é«˜çš„æº–ç¢ºç‡ï¼Œå¾—åˆ†ç‚º 0.9889ï¼Œå±•ç¤ºäº†å®ƒå€‘å°æ­¤è³‡æ–™é›†çš„æœ‰æ•ˆæ€§ã€‚éš¨æ©Ÿæ£®æ—ä¹Ÿè¡¨ç¾è‰¯å¥½ï¼Œå±•ç¤ºäº†æ•´é«”æ¨¡å‹çš„å„ªå‹¢ã€‚ç¥ç¶“ç¶²è·¯é›–ç„¶å¤šåŠŸèƒ½ï¼Œä½†é”åˆ°äº†è¼ƒä½çš„æº–ç¢ºç‡ 0.8260ï¼Œè¡¨æ˜éœ€è¦é€²ä¸€æ­¥èª¿æ•´ã€‚ç¸½é«”è€Œè¨€ï¼Œçµæœè¡¨æ˜ SVM å’Œé‚è¼¯è¿´æ­¸æ˜¯æ­¤ä»»å‹™çš„åˆé©é¸æ“‡ï¼Œä½†éš¨æ©Ÿæ£®æ—ç­‰å…¶ä»–æ¨¡å‹ä¹Ÿæä¾›äº†ç«¶çˆ­æ€§èƒ½ã€‚
'''



Academic_Report = """You need to write an academic data analysis report in markdown format based on what is within the dialog history. The report needs to contain the following (if present):
1. Title: The title of the report.
2. Abstract: Includes the background of the task, what datasets were used, data processing methods, what models were used, what conclusions were drawn, etc. It should be around 200 words.
3. Introduction: give the background to the task and the dataset, around 200 words.
4. Methodology: this section can be expanded according to the following subtitle. There is no limit to the number of words.
    (4.1) Dataset: introduce the dataset, include statistical description, characteristics and features of the dataset, the target, variable types, missing values and so on.
    (4.2) Data Processing: Includes all the steps taken by the user to process the dataset, what methods were used to process the dataset, and you can show 5 rows of data after processing. 
          Note: If any figure saved, you should include them in the document as well, use the link in the chat history, for example:
          ![figure.png](/path/to/the/figure.png).
    (4.3) Modeling: Includes all the models trained by the user, you can add some introduction to the algorithm of the model.
5. Results: This part is presented in tables as much as possible, containing all model evaluation metrics summarized in one table for comparison. There is no limit to the number of words.
6. conclusion: summarize this report, around 200 words.
Here is a figure list with links in the chat history for your reference : {figures}
Here is an example for you:

# Classification Task Using Wine Dataset with Machine Learning Models

## 1. Abstract:

This report outlines the process of building and evaluating multiple machine learning models for a classification task on the Wine dataset. The dataset was preprocessed by standardizing the features and ordinal encoding the target variable, "class." Various classification models were trained, including Logistic Regression, SVM, Decision Tree, Random Forest, Neural Networks, and ensemble methods like Bagging and XGBoost. Cross-validation and GridSearchCV were employed to optimize the hyperparameters of each model. Logistic Regression achieved an accuracy of 98.89%, while the best-performing models included Random Forest and SVM. The models' performances are compared, and their strengths are discussed, demonstrating the effectiveness of ensemble methods and support vector machines for this task.

## 2. Introduction

The task at hand is to perform a classification on the Wine dataset, a well-known dataset that contains attributes related to different types of wine. The goal is to correctly classify the wine type (target variable: "class") based on its chemical properties such as alcohol content, phenols, color intensity, etc. Machine learning models are ideal for this kind of task, as they can learn patterns from the data to make accurate predictions. This report details the preprocessing steps applied to the data, including standardization and ordinal encoding. It also discusses various machine learning models such as Logistic Regression, Decision Tree, SVM, and ensemble models, which were trained and evaluated using cross-validation. Additionally, GridSearchCV was employed to fine-tune model parameters to achieve optimal accuracy.

## 3. Methodology:

**3.1 Dataset:**
The Wine dataset used in this task contains 13 continuous features representing various chemical properties of wine, such as Alcohol, Malic acid, Ash, Magnesium, and Proline. The target variable, "class," is categorical and has three possible values, each corresponding to a different type of wine. A correlation matrix was generated to understand the relationships between the features, and standardization was applied to normalize the values. The dataset had no missing values.

**3.2 Data Processing:**

- Standardization: The features were standardized using `StandardScaler`, which adjusts the mean and variance of each feature to make them comparable.
- Ordinal Encoding: The target column, "class," was converted into numerical values using `OrdinalEncoder`.

|      | Alcohol  | Malicacid | Ash  | Alcalinity_of_ash | Magnesium | Total_phenols | Flavanoids | Nonflavanoid_phenols | Proanthocyanins | Color_intensity | Hue  | 0D280_0D315_of_diluted_wines | Proline | class |
| ---- | -------- | --------- | ---- | ----------------- | --------- | ------------- | ---------- | -------------------- | --------------- | --------------- | ---- | ---------------------------- | ------- | ----- |
| 0    | 1.518613 | -0.562250 | 0.23 | -1.169593         | 1.913905  | 0.808997      | 1.034819   | -0.659563            | 1.224884        | 0.251717        | 0.36 | 1.847920                     | 1.013   | 0     |

For visualization, a correlation matrix was generated to show how different features correlate with each other and with the target:

![sepal_length_distribution.png](/path/to/the/figure.png)

**3.3 Modeling:**
Several machine learning models were trained on the processed dataset using cross-validation for evaluation. The models include:

- **Logistic Regression**: A linear model suitable for binary and multiclass classification tasks.
- **SVM (Support Vector Machine)**: Known for handling high-dimensional data and effective in non-linear classifications when using different kernels.
- **Neural Network (MLPClassifier)**: A neural network model was tested with varying hidden layer sizes.
- **Decision Tree**: A highly interpretable model that splits the dataset recursively based on feature values.
- **Random Forest**: An ensemble of decision trees that reduces overfitting by averaging predictions from multiple trees.
- **Bagging**: An ensemble method to train multiple classifiers on different subsets of the dataset.
- **Gradient Boosting**: A sequential model that builds trees to correct previous errors, improving accuracy with each iteration.
- **XGBoost**: A gradient boosting technique optimized for performance and speed
- **AdaBoost**: An ensemble method that boosts weak classifiers by focusing more on incorrectly classified instances.

Each model's hyperparameters were optimized using `GridSearchCV`, and evaluation metrics such as accuracy were recorded.

## 4. Results:

The results of model evaluation are summarized below:

| Model               | Best Parameters                                              | Accuracy |
| ------------------- | ------------------------------------------------------------ | -------- |
| Logistic Regression | Default                                                      | 0.9889   |
| SVM                 | {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}                 | 0.9889   |
| Neural Network      | {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (3, 4, 3)} | 0.8260   |
| Decision Tree       | {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2} | 0.9214   |
| Random Forest       | {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 500} | 0.9833   |
| Bagging             | {'bootstrap': True, 'max_samples': 0.5, 'n_estimators': 100} | 0.9665   |
| GradientBoost       | {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100}  | 0.9665   |
| XGBoost             | {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}  | 0.9554   |
| AdaBoost            | {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 10} | 0.9389   |

## 5. Conclusion:

This report presents the steps and results of performing a classification task using various machine learning models on the Wine dataset. Logistic Regression and SVM yielded the highest accuracies, with scores of 0.9889, demonstrating their effectiveness for this dataset. Random Forest also performed well, showcasing the strength of ensemble models. Neural Networks, while versatile, achieved a lower accuracy of 0.8260, indicating the need for further tuning. Overall, the results suggest that SVM and Logistic Regression are suitable choices for this task, but additional models like Random Forest offer competitive performance.
"""

Experiment_Report = '''
You are a report writer. You need to write an data analysis experimental report in markdown format based on what is within the dialog history. The report needs to contain the following (if present):
1. Title: The title of the report.
2. Experiment Process: Includes all the useful processes of the task, You should give the following information for every step:
 (1) The purpose of the process
 (2) The code of the process (only correct code.), wrapped with ```python```.
       # Example of code snippet 
         ```python
         import pandas as pd
	     df = pd.read_csv('data.csv')
	     df.head()
         ```
 (3) The result of the process (if present).
       To show a figure or model, use ![figure.png](/path/to/the/figure.png).
4. Summary: Summarize all the above evaluation results in tabular format.
5. Conclusion: Summarize this report, around 200 words.
Here is a figure list with links in the chat history for your reference : {figures}
Here is an example for you: 
{example}
'''

SYSTEM_PROMPT_EDU = '''æ‚¨æ˜¯ä¸€ä½èª²ç¨‹è¨­è¨ˆå¸«ã€‚æ‚¨æ‡‰è©²ç‚ºä½¿ç”¨è€…è¨­è¨ˆèª²ç¨‹å¤§ç¶±å’Œä½œæ¥­ã€‚'''


KNOWLEDGE_INTEGRATION_SYSTEM = '''\næ­¤å¤–ï¼Œæ‚¨å¯ä»¥å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢ä¸€äº›çŸ¥è­˜çš„ç¨‹å¼ç¢¼ã€‚çŸ¥è­˜æœ‰å…©ç¨®æ¨¡å¼ï¼šä¸€ç¨®æ˜¯ã€Œå®Œæ•´ã€æ¨¡å¼ï¼Œé€™æ„å‘³è‘—æ•´å€‹ç¨‹å¼ç¢¼ç‰‡æ®µå°‡å‘ˆç¾çµ¦æ‚¨ã€‚æ‚¨æ‡‰è©²åƒè€ƒæ­¤ç¨‹å¼ç¢¼å˜—è©¦è§£æ±ºå•é¡Œã€‚ã€Œå®Œæ•´ã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼å°‡æ ¼å¼åŒ–ç‚ºï¼š
\nğŸ“ æª¢ç´¢ï¼š\næª¢ç´¢å™¨æ‰¾åˆ°äº†ä»¥ä¸‹å¯èƒ½æœ‰åŠ©æ–¼è§£æ±ºå•é¡Œçš„ç¨‹å¼ç¢¼ç‰‡æ®µã€‚æ‚¨æ‡‰è©²åƒè€ƒæ­¤ç¨‹å¼ç¢¼ä¸¦é©ç•¶ä¿®æ”¹å®ƒã€‚
ã€Œå®Œæ•´ã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼ï¼š
ç¨‹å¼ç¢¼æè¿°ï¼š{desc}
å®Œæ•´ç¨‹å¼ç¢¼ï¼š```{code}\n```\n
æ‚¨ä¿®æ”¹çš„ç¨‹å¼ç¢¼ï¼š

å¦ä¸€ç¨®æ¨¡å¼æ˜¯ã€Œæ ¸å¿ƒã€æ¨¡å¼ï¼Œé€™æ„å‘³è‘—ä¸€äº›å‡½æ•¸ç¨‹å¼ç¢¼å·²ç¶“è¢«å®šç¾©å’ŒåŸ·è¡Œã€‚æ‚¨å¯ä»¥ç›´æ¥åƒè€ƒå’Œä¿®æ”¹æ ¸å¿ƒç¨‹å¼ç¢¼ä¾†è§£æ±ºå•é¡Œã€‚è«‹æ³¨æ„ï¼Œæ‚¨æ‡‰è©²é¦–å…ˆæª¢æŸ¥å®šç¾©çš„ç¨‹å¼ç¢¼æ˜¯å¦å®Œå…¨æ»¿è¶³ä½¿ç”¨è€…çš„éœ€æ±‚ã€‚ã€Œæ ¸å¿ƒã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼å°‡æ ¼å¼åŒ–ç‚ºï¼š
\nğŸ“ æª¢ç´¢ï¼š\næª¢ç´¢å™¨æ‰¾åˆ°äº†ä»¥ä¸‹å¯ä»¥è§£æ±ºå•é¡Œçš„ç¨‹å¼ç¢¼ç‰‡æ®µã€‚æ‰€æœ‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œã€‚
ã€Œæ ¸å¿ƒã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼ï¼š
ç¨‹å¼ç¢¼æè¿°ï¼š{desc}
åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œçš„ç¨‹å¼ç¢¼ï¼ˆæª¢æŸ¥å®šç¾©çš„ç¨‹å¼ç¢¼æ˜¯å¦å®Œå…¨æ»¿è¶³ä½¿ç”¨è€…çš„éœ€æ±‚ï¼‰ï¼š```\n{back-end code}\n```\n
æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ˆåƒè€ƒæ­¤æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œè«‹æ³¨æ„æ‰€æœ‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒå€‘ï¼‰ï¼š\n```core_function\n{core}\n```\n
æ‚¨çš„ç¨‹å¼ç¢¼ï¼š


ä»¥ä¸‹æ˜¯æª¢ç´¢çŸ¥è­˜çš„ç¯„ä¾‹ï¼š
ä½¿ç”¨è€…ï¼šæˆ‘æƒ³ä½¿ç”¨äºŒæ¬¡æ”¶æ–‚ç‰›é “æ³•è¨ˆç®—æœ€è¿‘çš„ç›¸é—œçŸ©é™£ã€‚è«‹æ’°å¯«è©³ç´°çš„ç¨‹å¼ç¢¼ã€‚ç¨‹å¼ç¢¼æ‡‰æä¾›æ¯æ¬¡è¿­ä»£çš„è¨ˆç®—è©³ç´°è³‡è¨Šï¼Œä¾‹å¦‚æ¢¯åº¦çš„ç¯„æ•¸ã€ç›¸å°å°å¶é–“éš™ã€å°å¶ç›®æ¨™å‡½æ•¸å€¼ã€åŸå§‹ç›®æ¨™å‡½æ•¸å€¼å’Œé‹è¡Œæ™‚é–“ã€‚
ä½¿ç”¨ä»¥ä¸‹åƒæ•¸é‹è¡Œæ¸¬è©¦æ¡ˆä¾‹ä¸¦é¡¯ç¤ºçµæœï¼š
è¨­ç½®ä¸€å€‹ 2000x2000 éš¨æ©ŸçŸ©é™£ï¼Œå…¶å…ƒç´ å¾æ¨™æº–å¸¸æ…‹åˆ†ä½ˆä¸­éš¨æ©ŸæŠ½å–ï¼ŒçŸ©é™£æ‡‰è©²æ˜¯å°ç¨±æ­£åŠå®šçš„ã€‚
è¨­ç½® b å‘é‡ç‚º 2000x1ï¼Œæ‰€æœ‰å…ƒç´ ç‚º 1ã€‚
è¨­ç½® tau ç‚º 0.1ï¼Œå®¹å·®èª¤å·®ç‚º 1.0e-7ã€‚

æ‚¨çš„å›æ‡‰ï¼š
\nğŸ“ æª¢ç´¢ï¼š\næª¢ç´¢å™¨æ‰¾åˆ°äº†ä»¥ä¸‹å¯ä»¥è§£æ±ºå•é¡Œçš„ç¨‹å¼ç¢¼ç‰‡æ®µã€‚æ‰€æœ‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œã€‚
ã€Œæ ¸å¿ƒã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼ï¼š
ç¨‹å¼ç¢¼æè¿°ï¼š\næ­¤å‡½æ•¸ä½¿ç”¨äºŒæ¬¡æ”¶æ–‚ç‰›é “æ³•è¨ˆç®—æœ€è¿‘çš„ç›¸é—œçŸ©é™£ã€‚å¯æ¥å—çš„åƒæ•¸ï¼šSigmaã€b>0ã€tau>=0 å’Œ tolï¼ˆå®¹å·®èª¤å·®ï¼‰ã€‚å°æ–¼ç›¸é—œçŸ©é™£å•é¡Œï¼Œè¨­ç½® b = np.ones((n,1))ã€‚
åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œçš„ç¨‹å¼ç¢¼ï¼ˆæª¢æŸ¥å®šç¾©çš„ç¨‹å¼ç¢¼æ˜¯å¦å®Œå…¨æ»¿è¶³ä½¿ç”¨è€…çš„éœ€æ±‚ï¼‰ï¼š
```
def NearestCorrelationMatrix(self, g_input, b_input=None, tau=None, tol=None):
    print('-- Semismooth Newton-CG method starts -- \n')
    [n, m] = g_input.shape
    g_input = g_input.copy()
    t0 = time.time()  # time start
    g_input = (g_input + g_input.transpose()) / 2.0
    b_g = np.ones((n, 1))
    error_tol = 1.0e-6
    if b_input is None:
    ......
```


æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ˆåƒè€ƒæ­¤æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œè«‹æ³¨æ„ NearestCorrelationMatrix() ç­‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒå€‘ï¼‰ï¼š
```
# test
n = 3000
data_g_test = scipy.randn(n, n)
data_g_test = (data_g_test + data_g_test.transpose()) / 2.0
data_g_test = data_g_test - np.diag(np.diag(data_g_test)) + np.eye(n)
b = np.ones((n, 1))
tau = 0
tol = 1.0e-6
[x_test_result, y_test_result] = NearestCorrelationMatrix(data_g_test, b, tau, tol)
print(x_test_result)
print(y_test_result)
```

æ‚¨çš„ç¨‹å¼ç¢¼ï¼š
é¦–å…ˆï¼Œæˆ‘æª¢æŸ¥äº†æ‰€æœ‰å®šç¾©çš„ç¨‹å¼ç¢¼æ˜¯å¦æ»¿è¶³ä½¿ç”¨è€…çš„éœ€æ±‚ã€‚æˆ‘å¯ä»¥ç›´æ¥ä½¿ç”¨æ ¸å¿ƒç¨‹å¼ç¢¼ä¾†è§£æ±ºå•é¡Œã€‚

```
import numpy as np
from scipy import randn
# å®šç¾©è¼¸å…¥çŸ©é™£
n = 3000
data_g_test = np.random.randn(n, n)
data_g_test = (data_g_test + data_g_test.transpose()) / 2.0
data_g_test = data_g_test - np.diag(np.diag(data_g_test)) + np.eye(n)
# å®šç¾©åˆå§‹çŒœæ¸¬
b = np.ones((n, 1))
# å®šç¾©æ‡²ç½°åƒæ•¸å’Œå®¹å·®
tau = 0
tol = 1.0e-6
# å‘¼å« NearestCorrelationMatrix å‡½æ•¸ï¼ˆç›´æ¥ä½¿ç”¨ NearestCorrelationMatrix()ï¼‰
[x_test_result, y_test_result] = NearestCorrelationMatrix(data_g_test, b, tau, tol) 
print(x_test_result)
print(y_test_result)
```
'''


PMT_KNW_IN_FULL = """
\nğŸ“ æª¢ç´¢ï¼š\næª¢ç´¢å™¨æ‰¾åˆ°äº†ä»¥ä¸‹å¯èƒ½æœ‰åŠ©æ–¼è§£æ±ºå•é¡Œçš„ç¨‹å¼ç¢¼ç‰‡æ®µã€‚æ‚¨æ‡‰è©²åƒè€ƒæ­¤ç¨‹å¼ç¢¼ä¸¦é©ç•¶ä¿®æ”¹å®ƒã€‚
ã€Œå®Œæ•´ã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼ï¼š
ç¨‹å¼ç¢¼æè¿°ï¼š\n{desc}
å®Œæ•´ç¨‹å¼ç¢¼ï¼š\n```\n{code}\n```\n
æ‚¨ä¿®æ”¹çš„ç¨‹å¼ç¢¼ï¼š
"""


PMT_KNW_IN_CORE = """
\nğŸ“ æª¢ç´¢ï¼š\næª¢ç´¢å™¨æ‰¾åˆ°äº†ä»¥ä¸‹å¯ä»¥è§£æ±ºå•é¡Œçš„ç¨‹å¼ç¢¼ç‰‡æ®µã€‚æ‰€æœ‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œã€‚
ã€Œæ ¸å¿ƒã€æ¨¡å¼çš„æª¢ç´¢ç¨‹å¼ç¢¼ï¼š
ç¨‹å¼ç¢¼æè¿°ï¼š\n{desc}
åœ¨å¾Œç«¯å®šç¾©å’ŒåŸ·è¡Œçš„ç¨‹å¼ç¢¼ï¼ˆæª¢æŸ¥å®šç¾©çš„ç¨‹å¼ç¢¼æ˜¯å¦å®Œå…¨æ»¿è¶³ä½¿ç”¨è€…çš„éœ€æ±‚ï¼‰ï¼š\n```\n{code_backend}\n```\n
æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ˆåƒè€ƒæ­¤æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œè«‹æ³¨æ„æ‰€æœ‰å‡½æ•¸å’Œé¡éƒ½å·²åœ¨å¾Œç«¯å®šç¾©ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒå€‘ï¼‰ï¼š\n```\n{core}\n```\n
æ‚¨çš„ç¨‹å¼ç¢¼ï¼š
"""
